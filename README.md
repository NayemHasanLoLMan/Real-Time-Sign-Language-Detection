# âœ‹ Real-Time Sign Language Detection

This project is my **university final year project**, aiming to bridge the communication gap for the deaf and hard-of-hearing community by translating sign language into readable text in **real time**. The system leverages **Python**, **OpenCV**, and **MediaPipe** to detect and interpret **alphanumeric hand gestures** accurately and efficiently.

---

## ğŸ¯ Project Objective

To build a computer vision-based system that:
- Detects hand gestures via webcam
- Classifies them into alphanumeric signs (A-Z, 0-9)
- Displays the translated characters in real time
- Assists in communication for individuals with hearing/speech impairments

---

## ğŸ› ï¸ Technologies Used

- ğŸ **Python** â€“ Core programming language
- ğŸ–¼ï¸ **OpenCV** â€“ Real-time computer vision processing
- ğŸ–ï¸ **MediaPipe** â€“ Hand detection and tracking
- ğŸ§  **Custom ML Model** â€“ For gesture classification (pickle + `.p` model)
- ğŸ“¦ **NumPy, Pandas** â€“ Dataset handling and analysis

---

## ğŸ“ Project Structure

â”œâ”€â”€ CollectImage.py # Capture hand gesture images for the dataset
â”œâ”€â”€ CreateDataset.py # Preprocess and label captured data
â”œâ”€â”€ TrainClassifier.py # Train the gesture classification model
â”œâ”€â”€ InferenceClassifier.py # Real-time prediction using webcam
â”œâ”€â”€ model.p # Trained classifier model
â”œâ”€â”€ data.pickle # Label encoding and mappings
â”œâ”€â”€ README.md # Project documentation



---

## ğŸš€ How It Works

1. **CollectImage.py**  
   - Use webcam to capture hand gestures (A-Z, 0-9)  
   - Save the images in structured folders for dataset creation

2. **CreateDataset.py**  
   - Preprocess image data  
   - Create a training-ready dataset for the ML model

3. **TrainClassifier.py**  
   - Use the dataset to train a classifier (e.g., KNN, SVM, etc.)  
   - Save the model for inference

4. **InferenceClassifier.py**  
   - Live webcam feed + MediaPipe hand tracking  
   - Predicts gesture and shows the result in real-time

---

## ğŸ’¡ Key Features

- â±ï¸ **Real-Time Prediction** with frame-by-frame classification
- ğŸ§  **Custom Model Training** for alphanumeric hand signs
- ğŸ¥ **Live Webcam Support** with hand tracking
- ğŸŒ **Scalable** for future language/model extension (e.g., full ASL, BSL)

---

## ğŸ§ª Example Use Case

> A deaf user signs the letter â€œAâ€ using one hand before the camera.  
> The model detects hand keypoints using MediaPipe â†’ Classifies the pose â†’ Displays â€œAâ€ on screen.

---

## ğŸ“¸ Sample UI

> _(Insert screenshot or demo gif here)_  
> Example:  
> Real-time hand gesture detected â†’ Result printed â†’ Smooth and accurate UI experience

---

## ğŸ“Œ Future Improvements

- ğŸ”¤ Support for full ASL (not just A-Z/0-9)
- ğŸ“± Android App using MLKit + Flutter
- ğŸ—£ï¸ Voice conversion of recognized signs
- ğŸ§© Integrate with Google Translate API for multi-language support

---

## ğŸ§‘â€ğŸ“ Developed By

**Nayem Hasan**  
Final Year Student, Computer Science & Engineering  
Dhaka City College  
[GitHub](https://github.com/NayemHasanLoLMan)

---

## ğŸ“œ License

This open-source project is available under the [MIT License](LICENSE).

